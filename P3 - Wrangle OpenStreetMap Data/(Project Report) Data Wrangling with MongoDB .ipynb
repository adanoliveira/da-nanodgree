{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:grey'> Data Wrangling with MongoDB </span> \n",
    "______________________\n",
    "## <span style='color:orange'> Wrangling OpenStreetMap Data </span> \n",
    "### <span style='color:grey'> Adan Olivera </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <span style='color:orange'> Map Area: São Paulo, SP, Brazil </span> \n",
    "\n",
    "- [Map area in OpenStreetMap](https://www.openstreetmap.org/relation/298285)\n",
    "- [Metro extract in MapZen](https://mapzen.com/data/metro-extracts.html#sao-paulo_brazil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we're going to detail the relevant aspects of the Wrangle OpenStreetMap Data project from Udacity's Data Analyst Nanodegree.\n",
    "\n",
    "The goal of this project is to use data munging techniques (e.g. assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity) to clean OpenStreetMap data for a part of the world, and then import the cleansed data into a MongoDB database to run exploratory queries against it.\n",
    "\n",
    "The chosen place in the world for this project is the São Paulo region in Brazil because it is where I live. It's the main economic region of the country and the one with highest population, 20 million people as of 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "____________\n",
    "\n",
    "## <span style='color:orange'> Contents </span> \n",
    "\n",
    "\n",
    "1.Problems encountered in the map\n",
    "\n",
    "    1.1 Street names\n",
    "    1.2 Postal codes\n",
    "    1.3 Phone numbers\n",
    "   \n",
    "   \n",
    "2.Data Overview\n",
    "    \n",
    "    2.1 File sizes\n",
    "    2.2 Number of documents \n",
    "    2.3 Number of nodes\n",
    "    2.4 Number of ways\n",
    "    2.5 Number of unique users\n",
    "    2.6 Number of pizza or japanese restaurants in the area\n",
    "    \n",
    "    \n",
    "3.Additional Ideas\n",
    "    \n",
    "    3.1 Improving Cycling Information\n",
    "    3.2 Additional data exploration using MongoDB queries\n",
    "\n",
    "4.Conclusion\n",
    "\n",
    "5.References\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:orange'> 1. Problems Encountered in the Map </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the metro extact .osm file for the city of São Paulo and running it against some auditing scripts (listed in data_auditing_scripts.py), I noticed some validity, accuracy and uniformity problems with some sections of the data:\n",
    "1. **Street names:** some street names contained invalid street types and some were incomplete;\n",
    "2. **Post codes:** there innacurate post codes, missing or with extra characters. They were also disuniform and some even invalid;\n",
    "3. **Phone numbers:** I also found problems with the structure of phone numbers. There were innacurate cases, invalid ones and they generally lacked uniformity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:grey'> 1.1 Street names </span> \n",
    "\n",
    "From the auditing script detailed below (adpated from lesson 6), I could find the following problems with street name strings:\n",
    "\n",
    "- **Over-abbreviated street names**: there were several abbreviation variations for each expected street type, making the data disuniform. To fix this issue, I mapped all abreviation variations to their respective full type and, using a python script, simply replaced the substrings corresponding to the abbreviations in the problematic street names, such that \"Al. Santos\" would be converted into \"Alameda Santos\", as an example.\n",
    "\n",
    "- **Informal and unexpected street types**: there were elements with informal street types (e.g. \"passagem\", \"via\", \"acost\"), which rigorously wouldn't be used to represent street names. Since even being informal these types may be usefull, I chose to keep them as they were and simply add them to my expected types list.\n",
    "\n",
    "- **Incomplete street names**: some names had their type missing, and just contained the actual street name or only part of it. For these cases, I manually searched for their names on \"Google Maps\" to get their street types and ensure their accuracy. I then mapped them and using the same script used to fix abbreviations, I replaced the incomplete strings with accurate and valid ones, adding street types (e.g. \"Alfonso\" became \"Avenida Alfonso\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'1': set([u'1\\xaa Travessa da Estrada do Morro Grande']),\n",
      " '3': set(['3']),\n",
      " 'AC': set(['AC SAO BERNARDO DO CAMPO']),\n",
      " 'Acost': set(['Acost. Direita KM 12,0 /Marg.Tie. Expr.']),\n",
      " u'Al': set(['Al. Barros',\n",
      "             'Al. Jauaperi',\n",
      "             u'Al. Joaquim Eug\\xeanio de Lima',\n",
      "             u'Al. Jos\\xe9 Maria Lisboa',\n",
      "             'Al. Lorena',\n",
      "             'Al. Pamplona',\n",
      "             'Al. Santos',\n",
      "             u'Al. Sarutai\\xe1']),\n",
      " 'Alfonso': set(['Alfonso Bovero']),\n",
      " 'Antonio': set(['Antonio Caputo']),\n",
      " u'Av': set(['Av C',\n",
      "             'Av Dr. Silvio de Campos',\n",
      "             u'Av Guap\\xe9',\n",
      "             u'Av Jac\\xfa Pessego / Nova Trabalhadores',\n",
      "             u'Av. Agenor C. de Magalh\\xe3es',\n",
      "             u'Av. Ant\\xf4nio Joaquim de Moura Andrade',\n",
      "             'Av. Augusto Zorzi Baradel Furquim',\n",
      "             'Av. Comendador Masatoshi Shinmyo',\n",
      "             'Av. Francisco Matarazzo',\n",
      "             u'Av. Francisco N\\xf3brega Barbosa',\n",
      "             'Av. Presidente Juscelino Kubitschek',\n",
      "             u'Av. Prof. L\\xfacio Martins Rodrigues, travessas 4 e 5',\n",
      "             u'Av. das Na\\xe7\\xf5es Unidas']),\n",
      " 'Azevedo': set(['Azevedo Junior']),\n",
      " u'Caja\\xedba': set([u'Caja\\xedba']),\n",
      " 'Campo': set(['Campo Limpo Paulista']),\n",
      " u'Cant\\xeddio': set([u'Cant\\xeddio Miragaia']),\n",
      " u'Complexo': set([u'Complexo Vi\\xe1rio Engenheiro Job Shuji Nogami']),\n",
      " 'Coronel': set(['Coronel Euclides Machado',\n",
      "                 'Coronel Manuel Machado',\n",
      "                 'Coronel Melo Oliveira']),\n",
      " 'Corredor': set(['Corredor ABD']),\n",
      " 'Doutor': set(['Doutor Bento Teobaldo Ferraz']),\n",
      " 'Franklin': set(['Franklin do Amaral']),\n",
      " 'Garcia': set(['Garcia Lorca']),\n",
      " 'Guarara': set(['Guarara']),\n",
      " 'Manoel': set(['Manoel Ramos Paiva']),\n",
      " u'Marginal': set([u'Marginal Fern\\xe3o Dias']),\n",
      " 'Marina': set(['Marina', 'Marina Giacomini, 57']),\n",
      " u'N\\xedvia': set([u'N\\xedvia Maria Dombi']),\n",
      " u'Oscar': set([u'Oscar Foga\\xe7a']),\n",
      " 'Passagem': set(['Passagem das Flores', 'Passagem das Orquideas']),\n",
      " 'Pateo': set(['Pateo do Collegio']),\n",
      " 'Praca': set(['Praca Professor Reinaldo Porchat de Assis']),\n",
      " 'R': set(['R COJUBA',\n",
      "           'R RESTINGA',\n",
      "           u'R. Ant\\xf4nio Barroso Barreto',\n",
      "           'R. Peixe Vivo']),\n",
      " 'RUA': set(['RUA Jacques Felix']),\n",
      " 'RUa': set(['RUa Padre Chico']),\n",
      " 'Tavares': set(['Tavares Bastos']),\n",
      " 'Via': set(['Via Interna Posto BR',\n",
      "             'Via Interna do Frango Assado',\n",
      "             'Via Lateral (Norte)',\n",
      "             'Via Lateral (Sul)',\n",
      "             u'Via das Magn\\xf3lias',\n",
      "             u'Via de Liga\\xe7\\xe3o']),\n",
      " 'Viaduto': set([u'Viaduto Jacare\\xed',\n",
      "                 'Viaduto Pedroso',\n",
      "                 u'Viaduto do Ch\\xe1']),\n",
      " 'Vicente': set(['Vicente Ferreira Leite']),\n",
      " 'antonio': set(['antonio dos santos santinho',\n",
      "                 'antonio dos santos santinho, esplanada mendes, sao roque, sao paulo, brasil']),\n",
      " 'avenida': set(['avenida Sapopemba']),\n",
      " 'estrada': set(['estrada Tahira Eki', u'estrada jo\\xe3o lang']),\n",
      " 'rua': set(['rua',\n",
      "             'rua Alvaro Alvim',\n",
      "             'rua Carlos Lisdegno Carlucci',\n",
      "             'rua Heinrich Nordhoff',\n",
      "             u'rua Jos\\xe9 Ordonhes',\n",
      "             u'rua Jo\\xe3o Daprat',\n",
      "             'rua Lemos Torres',\n",
      "             'rua Oroganof',\n",
      "             'rua Secondo Modolin',\n",
      "             \"rua Sofia D'Angelo Caputo\",\n",
      "             'rua horacio cartier ',\n",
      "             'rua manoel  querinos de matos']),\n",
      " 'sao': set(['sao sebastiao'])}\n"
     ]
    }
   ],
   "source": [
    "### Script used to audit ways for unexpected types, listing the occurencies for each unexpected type in a set dictionary.\n",
    "### Output is a dictionary where the keys are the unexpected types and the values are sets of occurrences \n",
    "### for each unexpected type, as can be seen below.\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "OSMFILE = \"sao-paulo_brazil.osm\"\n",
    "street_type_re = re.compile(r'\\S+\\.?\\b', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Rua\", \"Avenida\", \"Alameda\", \"Quarteirão\", \"Quadra\", \"Lugar\", \"Viela\", \"Faixa\", \"Estrada\",\n",
    "                \"Trilha\", \"Praça\", \"Passarela\", 'Acesso', 'Largo', \"Rodovia\", \"Travessa\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type.encode('utf-8','ignore')  not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    return street_types\n",
    "\n",
    "st_types = audit(OSMFILE)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:grey'> 1.2 Post codes </span> \n",
    "\n",
    "From the post code auditing script shown below (adpated from lesson 6), I was able to notice different issues with post codes registered in the OSM file:\n",
    "\n",
    "- **Inconsistent formating**: many codes were represented with dots, additional or no hyphens and white spaces instead of only one hyphen as the national standard ('00000-000'). The exceptional values (with extra hyphens, dots, spaces) were mapped and replaced along with the simpler ones (only missing the hyphen) using a cleaning script in python.\n",
    "\n",
    "- **Invalid values**: some codes contained text instead of only digits (e.g. \"CEP\", \"Igreja Presbiteriana Vila Gustavo\"). As they weren't many, all of them were easily replaced by mapping the correct values either from Google searches or by intuition. There was also one post code apparently from a region outside the map analysed. Once the data was imported into MongoDB, I searched for the document containing that post code, and discovered that the error was just due to a typo. Then I updated its value with the correct one, using the commands below:\n",
    "\n",
    "```\n",
    "db.sao_paulo_brazil.find_one({\"address.postcode\":\"25450-000\"}) >> to find the document with the incorrect code\n",
    "\n",
    "db.sao_paulo_brazil.update({\"address.postcode\":\"25450-000\"},\n",
    "                           {\"$set\": {\"address.postcode\":\"02545-000\"}}) >> to replace the incorrect code\n",
    "\n",
    "```\n",
    "\n",
    "- **Incomplete or innacurate codes**: There were incomplete post codes (missing numbers) and some with extra digits. For most of them, I was able to find the correct code by searching for intuitive variations on the national postal agency website ([Correios](http://www.buscacep.correios.com.br/sistemas/buscacep/)). These were cases where an additional 0 was added to the code (e.g. '042010-000'), where there were 0s missing (e.g. '09380') or where the correct code was surrounded by incorrect digits (e.g. '09890-1 09890-080 00'). They were mapped and replaced used the cleaning script mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct': (8099,\n",
      "             ['03331-000',\n",
      "              '03302-000',\n",
      "              '03164-010',\n",
      "              '05461-010',\n",
      "              '02120-020',\n",
      "              '03403-003',\n",
      "              '06455-000',\n",
      "              '01301-000',\n",
      "              '01301-000',\n",
      "              '01318-001']),\n",
      " 'extra_chars': (97,\n",
      "                 ['010196-200',\n",
      "                  '12.216-540',\n",
      "                  '13.308-911',\n",
      "                  '023630000',\n",
      "                  '04783 020',\n",
      "                  '042010-000',\n",
      "                  '042010-000',\n",
      "                  '03032.030',\n",
      "                  '03032.030',\n",
      "                  '03032.030']),\n",
      " 'missing_chars': (3, ['09380', '05410', '12242']),\n",
      " 'missing_hyphen': (194,\n",
      "                    ['04345000',\n",
      "                     '12315280',\n",
      "                     '01309010',\n",
      "                     '01309000',\n",
      "                     '05006000',\n",
      "                     '01304001',\n",
      "                     '02615020',\n",
      "                     '05025010',\n",
      "                     '09930270',\n",
      "                     '05428000']),\n",
      " 'wrong_region': (1, ['25450-000'])}\n"
     ]
    }
   ],
   "source": [
    "### Script used to audit postcodes by grouping them into different problematic scenarios (e.g. extra or \n",
    "### missing characters), and listing examples for each case.\n",
    "### Output is a dictionary of tuples with one element being the count of occurrences in each scenario \n",
    "### and the other being a list of up to 10 examples.\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "OSMFILE = \"sao-paulo_brazil.osm\"\n",
    "post_code_re = re.compile(r'\\d{5}\\-\\d{3}', re.IGNORECASE)\n",
    "correct = []\n",
    "extra_chars = []\n",
    "missing_chars = []\n",
    "missing_hyphen = []\n",
    "wrong_region = []\n",
    "\n",
    "def audit_post_code(post_code_types, post_code):\n",
    "    post_code = post_code.encode('ascii','ignore') \n",
    "    if (\"-\" not in post_code) and (len(post_code) < 8):\n",
    "        missing_chars.append(post_code)\n",
    "        post_code_types[\"missing_chars\"] =(len(missing_chars), missing_chars[:10])\n",
    "    \n",
    "    elif ((\"-\" not in post_code) and (len(post_code) > 8) or (len(post_code) > 9)):\n",
    "        extra_chars.append(post_code)\n",
    "        post_code_types[\"extra_chars\"] = (len(extra_chars), extra_chars[:10])\n",
    "    \n",
    "    elif \"-\" not in post_code:\n",
    "        missing_hyphen.append(post_code)\n",
    "        post_code_types[\"missing_hyphen\"] = (len(missing_dash), missing_dash[:10])\n",
    "    \n",
    "    elif (post_code[0] != \"0\") and (post_code[0] != \"1\"):\n",
    "        wrong_region.append(post_code)\n",
    "        post_code_types[\"wrong_region\"] = (len(wrong_region), wrong_region[:10])\n",
    "    \n",
    "    elif re.search(post_code_re, post_code) is not None:\n",
    "        correct.append(post_code)\n",
    "        post_code_types[\"correct\"] = (len(correct), correct[:10])\n",
    "        \n",
    "def is_post_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    post_code_types = {\"missing_chars\":0, \"extra_chars\":0, \"missing_hyphen\":0, \"wrong_region\":0, \"correct\":0}\n",
    "    counter = 0\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_post_code(tag):\n",
    "                    audit_post_code(post_code_types, tag.attrib['v'])\n",
    "                    counter += 1\n",
    "    return post_code_types\n",
    "\n",
    "pc_types = audit(OSMFILE)\n",
    "pprint.pprint(pc_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:grey'> 1.3 Phone numbers </span> \n",
    "\n",
    "Phone numbers posed a more complex challange than the previous data types. Through the exploratory auditing script below, I was able to find many unexpected cases and numerous formatting variations. I experimented with different cleaning alternatives until I found a helpful python module called \"[phonenumbers](https://github.com/daviddrysdale/python-phonenumbers)\" with functions for treating phone strings. I then used it to parse the phone number strings into a consistend international format. The main problems encountered with phone numbers were the following:\n",
    "\n",
    "- **Inconsistent formating**: There were numbers with multiple hyphens (e.g. '55-11-37120713'), parenthesis (e.g. '+55 (11) 3583-1810'), dots (e.g. '011-2986.8540'), slashes (e.g. '+55 11 2949-1844 / 11 99602-0973'), white spaces (e.g. '+55 11 3322 2200') and some with none of these separators at all (e.g. '551151829947'). All these cases where converted into a consistent international format ('+00 00 0000-0000') using functions from the phonenumbers module in a custom python cleaning script.\n",
    "\n",
    "- **More than one number per tag**: Many phone number strings where actually composed of multiple phones numbers (e.g. '+55-11-32274554 +55-11-997537015' or '11 2959-3594 / 2977-2491'). These were also parsed using functions from the phonenumbers module and converted into a list with the individual numbers in international format as elements.\n",
    "\n",
    "- **Missing area codes**: Some numbers had area codes missing. Either they had the country code missing (e.g. '011-2986.8540') or the local area code missing (e.g. '5514-7964'). In some cases, the country code \"plus\" sign was missing e.g. 55-11-37120713) or even misplaced ('55+ (11) 3670-8000'). As the phonenumbers module can't correctly parse numbers without local codes, I then treated these strings to fix the problems before running them trough the parser. After being treated, they were parsed to the international format.\n",
    "\n",
    "- **Incomplete and innacurate numbers**: I also found numbers missing digits (e.g. '+55 11190' or '11193'), with additional digits (e.g. '+55 11 1 3135 4156') or with text among digits (e.g. '+55 11 2949-1844 / 11 99602-0973 com Sander'). As these weren't many, whenever was possible, they were mapped to their respective correct forms, and then fixed before being parsed. When there was text among the digits, a given function from the phonenumbers module was used to appropriately filter the numbers. For the cases where the number couldn't be guessed, they were parsed into empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_chars': (1204,\n",
      "                 ['+55 11 3104 0678',\n",
      "                  '0800 772 3633',\n",
      "                  '+55 11 33726800',\n",
      "                  '551151829947',\n",
      "                  '+55 11 3412 7611',\n",
      "                  '+55 11 22920977',\n",
      "                  '55 (11) 32592776',\n",
      "                  '+55 11 32728280',\n",
      "                  '+55 11 3289 1586',\n",
      "                  '+55 11 31710311']),\n",
      " 'missing_chars': (8,\n",
      "                   ['+55 11',\n",
      "                    '+55 11',\n",
      "                    '+55 11',\n",
      "                    '+55 11',\n",
      "                    '193',\n",
      "                    '190',\n",
      "                    '193',\n",
      "                    '+55 11']),\n",
      " 'missing_hyphen': (1, ['26455667']),\n",
      " 'other': (416,\n",
      "           ['+55 11 2292-2365',\n",
      "            '+55 13 3495-5504',\n",
      "            '+55 11 4648-1048',\n",
      "            '+55 11 4191-8707',\n",
      "            '+55 11 3255-2817',\n",
      "            '55-11-3222-1007',\n",
      "            '+55 11 2028-1010',\n",
      "            '+55 11 2692-0482',\n",
      "            '+55 11 2533-9791',\n",
      "            '+55 11 97753-0550'])}\n"
     ]
    }
   ],
   "source": [
    "### Script used to audit phone numbers by grouping them into different problematic scenarios (e.g. extra or missing characters), \n",
    "### and listing examples for each case.\n",
    "### Output is a dictionary of tuples with one element being the count of occurrences in each scenario and \n",
    "### the other being a list of up to 10 examples.\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "OSMFILE = \"sao-paulo_brazil.osm\"\n",
    "\n",
    "other = []\n",
    "extra_chars = []\n",
    "missing_chars = []\n",
    "missing_hyphen = []\n",
    "\n",
    "def audit_phone_number(phone_number_types, phone_number):\n",
    "    if (\"-\" not in phone_number) and (len(phone_number) < 8):\n",
    "        missing_chars.append(phone_number)\n",
    "        phone_number_types[\"missing_chars\"] =(len(missing_chars), missing_chars[:10])\n",
    "\n",
    "    elif (\"-\" not in phone_number) and (len(phone_number) > 8):\n",
    "        extra_chars.append(phone_number)\n",
    "        phone_number_types[\"extra_chars\"] = (len(extra_chars), extra_chars[:10])\n",
    "\n",
    "    elif \"-\" not in phone_number:\n",
    "        missing_hyphen.append(phone_number)\n",
    "        phone_number_types[\"missing_hyphen\"] = (len(missing_hyphen), missing_hyphen[:10])\n",
    "\n",
    "    else:\n",
    "        other.append(phone_number)\n",
    "        phone_number_types[\"other\"] = (len(other), other[:10])\n",
    "\n",
    "\n",
    "def is_phone_number(elem):\n",
    "    return (elem.attrib['k'] == \"phone\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    phone_number_types = {\"missing_chars\":0, \"extra_chars\":0, \"missing_hyphen\":0, \"other\":0}\n",
    "    counter = 0\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_phone_number(tag):\n",
    "                    counter += 1\n",
    "                    audit_phone_number(phone_number_types, tag.attrib['v'])\n",
    "    return phone_number_types\n",
    "\n",
    "\n",
    "pn_types = audit(OSMFILE)\n",
    "pprint.pprint(pn_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:orange'> 2. Data Overview </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running auditing scripts and mapping problems to be fixed, I defined a set of scripts to clean the data before importing it into MongoDB.\n",
    "\n",
    "To import the OSM XML, I firt converted it into a JSON file and then used the mongoimport tool to bulk insert its documents into a database named \"osm\" and a collection name \"sao_paulo_brazil\".\n",
    "\n",
    "With the XML to JSON convertion scripts I also run data cleaning scripts to correct the problems mentioned in the first section, before writing the values in the converted JSON. Only the elements of type “node” and “way” were imported, and the data model used for the documents follows the example below:\n",
    "\n",
    "```\n",
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type: \"node\",\n",
    "\"visible\":\"true\",\n",
    "\"created\": {\n",
    "          \"version\":\"2\",\n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "          \"user\":\"linuxUser16\",\n",
    "          \"uid\":\"1219059\"\n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"60625\",\n",
    "          \"street\": \"North Lincoln Ave\"\n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"1 (773)-271-5176\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data was imported, I run some queries to explore it. Here are some basic statistics extracted in this exploration, and the queries used to gather them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining functions to be used for queries\n",
    "\n",
    "def get_db(db_name):\n",
    "    #creates a connection and selects a database\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    #runs the aggregation pipeline and iterates through documents in it\n",
    "    return [doc for doc in db.sao_paulo_brazil.aggregate(pipeline)]\n",
    "\n",
    "db = get_db('osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 2.1 File sizes </span> \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "> sao-paulo_brazil.osm ......... 389.3 MB\n",
    "> sao-paulo_brazil.osm.json .... 562.1 MB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 2.2 Number of documents </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999296"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sao_paulo_brazil.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 2.3 Number of nodes </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1757483"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sao_paulo_brazil.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 2.4 Number of ways </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241756"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sao_paulo_brazil.find({\"type\":\"way\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 2.5 Number of unique users </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1655"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.sao_paulo_brazil.distinct(\"created.user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 2.6 Number of pizza or japanese restaurants in the area </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sao_paulo_brazil.find({\"amenity\":\"restaurant\",\n",
    "                              \"cuisine\": {\"$in\": [\"pizza\", \"japanese\"]}\n",
    "                             }).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:orange'> 3. Additional Ideas </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 3.1 Improving Cycling Information </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycling information for this region of the map is relatively very scarce. Only about 2% of ways have bycicle information of some type. And only 82 of places have registered bicycle parking, a mere 0.8% of the total number of amenities.\n",
    "\n",
    "Given the importance of bycicles as a clean trasportation altertive and as a solution to the local traffic problems, it would be beneficial to make sure that all ways have relevant bycicle information. Increasing the quality and volume of cycling information would make it easier to use bycicle in the city and would enable the development of software applications to support its use.\n",
    "\n",
    "There are some alternatives that would enables us to bridge this gap:\n",
    "\n",
    "- The solution should obviously involve the OSM local community or local software engineers motivated by the cause. We could contact local cycling groups and find engineers or developers among them that wish to volunteer and help.\n",
    "- The city government could be another party that could contribute promoting awareness or even with its resources. It has demosntrated interest in the subject with recent cycling projects in the city, like the construction of cyclelanes.\n",
    "- Local cycling-related businesses is another group that could also help. They have a direct interest in the increase of bycicle use and could contribute with resources or increasing awareness in the community.\n",
    "\n",
    "With volunteers or people commercially involved in improving cycling data in the region, the task could initiated by first listing the ways without bicycle information. Then starting with the ways with the highest traffic, they could start to map the cycling infrastructure and to populate OSM with their findings. Another way to tackle this would be by crowdsourcing the input of information to cyclists by a smartphone app, for example. OpenStreeMap provides [guidelines in their wiki](https://wiki.openstreetmap.org/wiki/Bicycle) for uploading bicycle information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - Number and share of ways with bicycle information </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bicyle tags is 4178\n",
      "bicycle information as a percentage of ways is 1.7282%\n"
     ]
    }
   ],
   "source": [
    "number_ways = db.sao_paulo_brazil.find({\"type\":\"way\"}).count()\n",
    "number_bicycle = db.sao_paulo_brazil.find({\"type\":\"way\",\"bicycle\":{\"$exists\":1}}).count()\n",
    "share_ways_bicycle = number_bicycle/float(number_ways)\n",
    "print \"number of bicyle tags is %1.0f\" %(number_bicycle) \n",
    "print \"bicycle information as a percentage of ways is %1.4f\" %(share_ways_bicycle*100) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - Number and share of ways with cyclelanes </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clycleways is 798\n",
      "clycleways as a percentage of ways is 0.3301%\n"
     ]
    }
   ],
   "source": [
    "number_high_cycleways = db.sao_paulo_brazil.find({\"type\":\"way\",\"highway\":\"cycleway\"}).count()\n",
    "number_way_cycleways = db.sao_paulo_brazil.find({\"type\":\"way\",\"cycleway\":{\"$exists\":1}}).count()\n",
    "number_cycleways = number_high_cycleways + number_way_cycleways\n",
    "share_ways_cycleways = number_cycleways/float(number_ways)\n",
    "print \"number of clycleways is %1.0f\" %(number_cycleways) \n",
    "print \"clycleways as a percentage of ways is %1.4f\" %(share_ways_cycleways*100) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - Number of places with bicycle parking, bicycle rental and compressed air </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of places with bicycle parking is 82\n",
      "number of places with bicycle rental is 95\n",
      "number of places with compressed air is 1\n"
     ]
    }
   ],
   "source": [
    "print \"number of places with bicycle parking is %1.0f\" %(db.sao_paulo_brazil.find({\"amenity\":\"bicycle_parking\"}).count())\n",
    "print \"number of places with bicycle rental is %1.0f\" %(db.sao_paulo_brazil.find({\"amenity\":\"bicycle_rental\"}).count())\n",
    "print \"number of places with compressed air is %1.0f\" %(db.sao_paulo_brazil.find({\"amenity\":\"compressed_air\"}).count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:grey'> 3.2 Additional data exploration using MongoDB queries </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - Top 10 amenities in the region </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'fuel', u'count': 1455},\n",
      " {u'_id': u'parking', u'count': 1140},\n",
      " {u'_id': u'restaurant', u'count': 930},\n",
      " {u'_id': u'school', u'count': 843},\n",
      " {u'_id': u'bank', u'count': 768},\n",
      " {u'_id': u'place_of_worship', u'count': 487},\n",
      " {u'_id': u'pharmacy', u'count': 349},\n",
      " {u'_id': u'fast_food', u'count': 336},\n",
      " {u'_id': u'pub', u'count': 269},\n",
      " {u'_id': u'hospital', u'count': 248}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$amenity\",\"count\":{\"$sum\":1}}}, \n",
    "            {\"$sort\":{\"count\":-1}}, \n",
    "            {\"$limit\":10}]\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - Top 10 leisure options in the region </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'park', u'count': 2498},\n",
      " {u'_id': u'pitch', u'count': 1590},\n",
      " {u'_id': u'swimming_pool', u'count': 336},\n",
      " {u'_id': u'sports_centre', u'count': 261},\n",
      " {u'_id': u'garden', u'count': 132},\n",
      " {u'_id': u'playground', u'count': 94},\n",
      " {u'_id': u'stadium', u'count': 40},\n",
      " {u'_id': u'common', u'count': 23},\n",
      " {u'_id': u'recreation_ground', u'count': 22},\n",
      " {u'_id': u'track', u'count': 16}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"leisure\":{\"$exists\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$leisure\", \"count\":{\"$sum\":1}}},        \n",
    "            {\"$sort\":{\"count\":-1}}, \n",
    "            {\"$limit\":10}]\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - Top 10 types of shopping places </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'supermarket', u'count': 728},\n",
      " {u'_id': u'yes', u'count': 711},\n",
      " {u'_id': u'bakery', u'count': 389},\n",
      " {u'_id': u'car', u'count': 217},\n",
      " {u'_id': u'car_repair', u'count': 191},\n",
      " {u'_id': u'clothes', u'count': 176},\n",
      " {u'_id': u'convenience', u'count': 162},\n",
      " {u'_id': u'mall', u'count': 137},\n",
      " {u'_id': u'hardware', u'count': 117},\n",
      " {u'_id': u'fashion', u'count': 99}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"shop\":{\"$exists\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$shop\", \"count\":{\"$sum\":1}}},        \n",
    "            {\"$sort\":{\"count\":-1}}, \n",
    "            {\"$limit\":10}]\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - Top 10 building types </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'house', u'count': 5096},\n",
      " {u'_id': u'apartments', u'count': 1774},\n",
      " {u'_id': u'residential', u'count': 1666},\n",
      " {u'_id': u'industrial', u'count': 1471},\n",
      " {u'_id': u'roof', u'count': 1295},\n",
      " {u'_id': u'commercial', u'count': 455},\n",
      " {u'_id': u'warehouse', u'count': 405},\n",
      " {u'_id': u'school', u'count': 241},\n",
      " {u'_id': u'retail', u'count': 192},\n",
      " {u'_id': u'public', u'count': 173}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"building\":{\"$exists\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$building\",\"count\":{\"$sum\":1}}}, \n",
    "            {\"$sort\":{\"count\":-1}}, \n",
    "            {\"$limit\":11},\n",
    "            {\"$skip\":1}]\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style='color:grey'> - 5 most popular cuisines in the region </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'regional', u'count': 136},\n",
      " {u'_id': u'pizza', u'count': 82},\n",
      " {u'_id': u'japanese', u'count': 52},\n",
      " {u'_id': u'italian', u'count': 23},\n",
      " {u'_id': u'burger', u'count': 18}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}, \n",
    "                       \"amenity\":\"restaurant\"}}, \n",
    "            {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},        \n",
    "            {\"$sort\":{\"count\":-1}}, \n",
    "            {\"$limit\":6},\n",
    "            {\"$skip\":1}]\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:orange'> 4. Conclusion </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this project we have seen that the OpenStreetMap is a powerful tool to explore maps, as it's open source, well documented, and though it's far from complete, it has very detailed information. \n",
    "\n",
    "Given that's an open platform, different people contribute in different ways and many times with wrong, mistyped or incomplete data, leading to inacurate, invalid or disuniform datasets. Part of the dirt data in this extract, more speceficaly street names, postcodes and phone numbers, were cleansed and standardized during this project, making the data for the city of São Paulo more accurate and uniform. \n",
    "\n",
    "With the power and simplicity of MongoDB, we were able to explore the region discovering interesting stats about it and revealing its scarcity of cycling related data, which could be used to improve transportation in the region if it were more complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:orange'> 5. References </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [OpenStreet Map Wiki](http://wiki.openstreetmap.org/wiki/Main_Page)\n",
    "- [Correios website](http://www.buscacep.correios.com.br/sistemas/buscacep/)\n",
    "- [Bicycles section in OpenStreetMap's wiki](https://wiki.openstreetmap.org/wiki/Bicycle)\n",
    "- [phonenumbers Python Library](https://github.com/daviddrysdale/python-phonenumbers)\n",
    "- [Google Maps](https://www.google.com.br/maps)\n",
    "- [MongoDB 3.2 Manual](https://docs.mongodb.org/manual/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
